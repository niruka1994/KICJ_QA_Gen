import streamlit as st
import time
import os
import tiktoken
import pandas as pd
from io import BytesIO
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import KonlpyTextSplitter, RecursiveCharacterTextSplitter

from dotenv import load_dotenv
from langchain_core.prompts import PromptTemplate
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

load_dotenv()

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
LANGCHAIN_PROJECT = os.environ.get('LANGCHAIN_PROJECT')


def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)


default_instruction = """
Exclude questions and answers related to tables, ratios, and numbers.
Exclude questions and answers related to the location of the page and the table of contents.
Restrict the question(s) to the context information provided only.
Based on the example below, you need to generate questions and answers in Korean.
"""

default_example = """
#EXAMPLE:
QUESTION: ë³€í˜¸ì‚¬ì™€ ì˜ë¢°ì¸ ê°„ ë¹„ë°€ìœ ì§€ê¶Œì˜ ì£¼ìš” ê°œë…ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ?
ANSWER: ë³€í˜¸ì‚¬ì™€ ì˜ë¢°ì¸ ê°„ ë¹„ë°€ìœ ì§€ê¶Œì€ ë³€í˜¸ì‚¬ì™€ ì˜ë¢°ì¸ ì‚¬ì´ì— ì£¼ê³ ë°›ì€ ì˜ì‚¬ì†Œí†µì— ê´€í•œ ì‚¬ì‹¤ì´ë‚˜ ìë£Œê°€ ë²•ì •ì— ì œì¶œ ë˜ëŠ” ê³µê°œë˜ëŠ” ê²ƒì„ ê±°ë¶€í•  ìˆ˜ ìˆëŠ” ê¶Œë¦¬ì…ë‹ˆë‹¤.

QUESTION: ìš°ë¦¬ë‚˜ë¼ì—ì„œ ë¹„ë°€ìœ ì§€ê¶Œ ë„ì…ì´ í•„ìš”í•œ ì´ìœ ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?
ANSWER: ë¹„ë°€ìœ ì§€ê¶Œ ë„ì…ì€ ì˜ë¢°ì¸ì˜ ê¶Œë¦¬ë³´í˜¸ì™€ ê¸€ë¡œë²Œ ìŠ¤íƒ ë‹¤ë“œì— ë§ì¶˜ ë²•ì¹˜ì£¼ì˜ì™€ ì ë²•ì ˆì°¨ì˜ ë‚´ì‹¤í™”ë¥¼ ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.

QUESTION: ë³€í˜¸ì‚¬ë²• ì œ26ì¡°ì— ë”°ë¥´ë©´ ë³€í˜¸ì‚¬ì˜ ë¹„ë°€ìœ ì§€ ì˜ë¬´ëŠ” ì–´ë–»ê²Œ ê·œì •ë˜ì–´ ìˆìŠµë‹ˆê¹Œ?
ANSWER: ë³€í˜¸ì‚¬ë²• ì œ26ì¡°ëŠ” ë³€í˜¸ì‚¬ê°€ ì§ë¬´ìƒ ì•Œê²Œ ëœ ë¹„ë°€ì„ ìœ ì§€í•´ì•¼ í•œë‹¤ê³  ëª…ì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.
"""

st.set_page_config(
    page_title="KICJ Q/A Gen",
    page_icon=":slightly_smiling_face:")

st.markdown("""
    <style>
        .sub-header {
            font-size: 18px;
            font-weight: bold;
            color: #696363;
            text-align: center;
        }
        .warning-text {
            font-size: 18px;
            color: red;
            font-weight: bold;
            text-align: center;
        }
        .center-text {
            text-align: center;
            font-size: 50px;
            font-weight: bold;
        }
        .custom-divider-rainbow {
        margin: 2em 0;
        height: 2px;
        background: linear-gradient(to right, 
            #FF0000, #FFA500, #FFFF00, 
            #008000, #0000FF, #4B0082, #EE82EE);
        }
        .custom-divider-gray {
        margin: 2em 0;
        height: 2px;
        background: #e8e7e6;
        }
    </style>
""", unsafe_allow_html=True)

st.markdown('<div class="center-text">ğŸ™‚ KICJ Q/A Gen</div>', unsafe_allow_html=True)
st.markdown('<div class="custom-divider-rainbow"></div>', unsafe_allow_html=True)
st.markdown('<div class="sub-header">ì´ í”„ë¡œê·¸ë¨ì€ GPTë¥¼ í†µí•˜ì—¬ PDF íŒŒì¼ì„ Q/A ë°ì´í„°ë¡œ ë³€í™˜í•´ì£¼ëŠ” ì‚¬ì´íŠ¸ ì…ë‹ˆë‹¤.</div>', unsafe_allow_html=True)
st.markdown('<div class="warning-text">AIëŠ” í—ˆìœ„ì •ë³´ë‚˜ ì˜¤ë¥˜ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìœ¼ë‹ˆ, í•­ìƒ ì¶”ê°€ ê²€ì¦ì„ ì§„í–‰í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.</div>', unsafe_allow_html=True)
st.markdown('<div class="custom-divider-rainbow"></div>', unsafe_allow_html=True)

uploaded_file = st.file_uploader("Upload Your PDF", type="pdf")

col1, col2 = st.columns([1, 1])
with col1:
    chunking_strategy = st.radio("Chunk Strategy (Optional)", ("Recursive", "Konlpy"),
                                 help="í˜ì´ì§€ë¥¼ ì¼ë ¨ì˜ ë¬¶ìŒ(ì²­í¬)ìœ¼ë¡œ ë¶„í• í•˜ì—¬ Q/Aë¥¼ ìƒì„±í•˜ê²Œ ë©ë‹ˆë‹¤. *Recursive = ë¬¸ë‹¨, Konlpy = í˜•íƒœì†Œ")

with col2:
    model_choice = st.radio("AI Model (Optional)", ("GPT-3.5-Turbo", "GPT-4o"))

if model_choice == 'GPT-3.5-Turbo':
    model_choice = 'gpt-3.5-turbo'
elif model_choice == 'GPT-4o':
    model_choice = 'gpt-4o'

input_chunk_size = st.slider("Chunk Size (Optional)", 500, 10000, 3000,
                             help="ì²­í¬ì˜ ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•©ë‹ˆë‹¤.")
input_chunk_overlap = st.slider("Chunk Overlap Size (Optional)", 50, 1000, 300,
                                help="ì²­í¬ê°„ ê²¹ì³ì§€ëŠ” ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•©ë‹ˆë‹¤.")
num_questions = st.slider("Q/A Per (Optional)", 1, 10, 3,
                          help="ì²­í¬ë³„ ìƒì„±ë˜ëŠ” Q/A ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. *ì²­í¬ ë‚´ìš©ì´ ë¶€ì¡±í•œê²½ìš°, ì„¤ì •í•œ ìˆ˜ì— ë„ë‹¬í•˜ì§€ ëª»í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.")
custom_instruction = st.text_area("Instruction (Optional)", value=default_instruction,
                                  height=150, help="AIì—ê²Œ ì§€ì‹œí•  ì œì•½ì¡°ê±´ì„ ì„œìˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
custom_example = st.text_area("Sample Q/A for AI (Optional)", value=default_example, height=150,
                              help="AIê°€ Q/A ìƒì„±ì— ì°¸ê³ í•  ì˜ˆì œë¥¼ ì„œìˆ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. *ì˜ˆì œì˜ ë‚´ìš©ë³´ë‹¨, ë¬¸ì¥ì˜ í˜•íƒœ(í‹€)ë¥¼ ì°¸ê³ í•©ë‹ˆë‹¤.")

if chunking_strategy == 'Recursive':
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=input_chunk_size, chunk_overlap=input_chunk_overlap,
                                                   length_function=tiktoken_len)
elif chunking_strategy == 'Konlpy':
    text_splitter = KonlpyTextSplitter(chunk_size=input_chunk_size, chunk_overlap=input_chunk_overlap,
                                       length_function=tiktoken_len)

st.markdown('<div class="custom-divider-gray"></div>', unsafe_allow_html=True)

process_button = st.button("Process")

prompt_template = f"""
Context information is below. You are only aware of this context and nothing else.
---------------------

{{context}}

---------------------
Given this context, generate only questions based on the below query.
You are an Teacher/Professor in Legal.
Your task is to provide exactly **{{num_questions}}** question(s) for an upcoming quiz/examination.
You are not to provide more or less than this number of questions.
The question(s) should be diverse in nature across the document.
The purpose of question(s) is to test the understanding of the students on the context information provided.
{custom_instruction}
{custom_example}
"""

prompt = PromptTemplate.from_template(prompt_template)

if uploaded_file is not None and process_button:
    with st.spinner("Processing..."):
        output_dir = os.path.join(os.getcwd(), "PDF")
        os.makedirs(output_dir, exist_ok=True)

        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        new_folder_path = os.path.join(output_dir, timestamp)
        os.makedirs(new_folder_path, exist_ok=True)

        file_name = uploaded_file.name
        tmp_file_path = os.path.join(new_folder_path, file_name)

        with open(tmp_file_path, "wb") as tmp_file:
            tmp_file.write(uploaded_file.read())

        pdf = PyPDFLoader(tmp_file_path)
        documents = pdf.load()

        combined_text = "".join(doc.page_content for doc in documents)
        split_documents = text_splitter.split_text(combined_text)

        llm = ChatOpenAI(
            model=model_choice,
            streaming=True,
            temperature=0
        )
        chain = (
                prompt |
                llm
        )

        placeholder = st.empty()

        qa_pairs = []

        for idx, document in enumerate(split_documents):
            if document:
                try:
                    response = chain.invoke(
                        {"context": document, "num_questions": num_questions}
                    )
                    st.markdown("**************************")
                    st.markdown(f"**Chunk {idx + 1}:**", help=document)
                    st.markdown(f"{response.content}")
                    chunk_text = document
                    qa_generated = response.content
                    vectorizer = CountVectorizer().fit_transform(
                        [chunk_text, qa_generated])
                    vectors = vectorizer.toarray()
                    cosine_sim_author = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
                    st.markdown(f"***ì²­í¬ì™€ ìƒì„±ëœ Q/Aì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ : {cosine_sim_author}***")
                    qa_pairs.append(response)

                    with placeholder.container():
                        st.markdown(
                            f"""
                            <div style='border: 1px solid #ccc; padding: 10px; border-radius: 5px; background-color: #f9f9f9;'>
                                <h4>Completed Chunk {idx + 1}:</h4>
                                <p>{response.content}</p>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                    time.sleep(1)
                except KeyError as e:
                    st.error(f"Error processing question {idx + 1}: {e}")
                except Exception as e:
                    st.error(f"An error occurred: {e}")

        # ë°ì´í„° ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        chunk_data = []

        # ì²­í¬ë³„ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        for idx, response in enumerate(qa_pairs):
            chunk_context = split_documents[idx]
            qa_pairs_split = response.content.split('QUESTION:')

            for qa_pair in qa_pairs_split[1:]:
                question_answer = qa_pair.split('ANSWER:')
                if len(question_answer) == 2:
                    question = question_answer[0].strip()
                    answer = question_answer[1].strip()
                    chunk_data.append({
                        'Chunk ë²ˆí˜¸': idx + 1,
                        'Chunk Context': chunk_context,
                        'QUESTION': question,
                        'ANSWER': answer
                    })

        # pandas DataFrame ìƒì„±
        df = pd.DataFrame(chunk_data)

        # ì—‘ì…€ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ê¸° ìœ„í•œ BytesIO ê°ì²´ ìƒì„±
        excel_buffer = BytesIO()
        df.to_excel(excel_buffer, index=False, engine='openpyxl')

        # ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        st.download_button(
            label="ì—‘ì…€íŒŒì¼ ì €ì¥(*ì €ì¥ì‹œ ë³¸ í˜ì´ì§€ ì´ˆê¸°í™”)",
            data=excel_buffer,
            file_name=f"Q_A_Generated_{timestamp}.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
